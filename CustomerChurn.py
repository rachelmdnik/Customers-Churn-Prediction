# -*- coding: utf-8 -*-
"""Swedia week 6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mfWjTyAMYzLuJhufHrE4EiUs41KE6QqH

# 1. Import & Load data

## Import Library
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('fivethirtyeight')
import warnings
import ast
warnings.filterwarnings('ignore')
# %matplotlib inline

# Agar semua kolom terlihat
pd.set_option('display.max_columns', None)

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score, KFold, train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.feature_selection import SelectKBest
from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, roc_auc_score
from xgboost import XGBClassifier
from sklearn import metrics
from sklearn.metrics import roc_curve
from sklearn.metrics import recall_score, confusion_matrix, precision_score, f1_score, accuracy_score, classification_report
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, ClassifierMixin

### Generating random color
### untuk visualisasi
import random
def hex_code_colors():
    a = hex(random.randrange(0,256))
    b = hex(random.randrange(0,256))
    c = hex(random.randrange(0,256))
    a = a[2:]
    b = b[2:]
    c = c[2:]
    if len(a)<2:
        a = "0" + a
    if len(b)<2:
        b = "0" + b
    if len(c)<2:
        c = "0" + c
    z = a + b + c
    return "#" + z.upper()
def list_color(n):
  hasil = [hex_code_colors() for i in range(n)]
  return hasil

# tes
list_color(2)

"""## Load Data

"""

# Load the Drive helper and mount
from google.colab import drive

# This will prompt for authorization.
drive.mount('/content/drive')

import os
for dirname, _, filenames in os.walk('/content/drive/MyDrive/Final Project DS/Mockup Dataset'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

raw_path = '/content/drive/MyDrive/Final Project DS/Mockup Dataset/'

"""## Product"""

### Terdapat error saat membaca product.csv dengan cara biasa
### Untuk mengatasinya, digunakan data movement
### Loop the data lines
with open(raw_path + 'product.csv', 'r') as data:
    # get No of columns in each line
    col_count = [ len(l.split(",")) for l in data.readlines() ]

### Generate column names  (names will be 0, 1, 2, ..., maximum columns - 1)
column_names = [i for i in range(0, max(col_count))]

### Read csv
product_df = pd.read_csv(raw_path + 'product.csv', header=None, delimiter=",", names=column_names)

product_df.head()

product_df.info()

product_df.drop([10,11], axis=1, inplace=True)

new_header = product_df.iloc[0] #grab the first row for the header
product_df = product_df[1:] #take the data less the header row
product_df.columns = new_header #set the header row as the df header
product_df.rename(columns={'id': 'product_id'}, inplace=True)
product_df.head()

product_df = product_df.reset_index()
product_df.info()

product_df.drop(product_df.columns[0], axis=1, inplace=True)
product_df.head()

"""### Deskripsi kolom:
- product_id : id unik produk <br>
- gender : gender produk <br>
- masterCategory : kategori utama produk <br>
- subCategory : kategori bagian produk <br>
- articleType : <br>
- baseColour : warna dasar dari produk <br>
- season : musim  <br>
- year : tahun rilis produk <br>
- usage : penggunaan produk <br>
- productDisplayName : Nama produk yang ditampilkan <br>

### Handling missing value
"""

### Melihat kolom yang memiliki missing value
product_df.isna().sum()

### fill missing value
product_df["baseColour"].fillna("Other", inplace = True)
product_df["season"].fillna("Unknown", inplace = True)
product_df["usage"].fillna("Other", inplace = True)
product_df["productDisplayName"].fillna("Blank", inplace = True)
product_df["year"].fillna(product_df['year'].mode()[0], inplace = True)

product_df['year'] = product_df['year'].astype('datetime64')
product_df['year'] = product_df['year'].dt.to_period('Y')

product_df[
    [
        "product_id",
        "gender",
        "masterCategory",
        "subCategory",
        "articleType",
        "baseColour",
        "season",
        "usage",
    ]
] = product_df[
    [
        "product_id",
        "gender",
        "masterCategory",
        "subCategory",
        "articleType",
        "baseColour",
        "season",
        "usage",
    ]
].astype(
    "category"
)

product_df.info()

"""### Visualisasi"""

### Melihat persebaran produk berdasarkan gender
tmp = pd.crosstab(index = product_df['gender'],
            columns = 'jumlah').sort_values(by='jumlah', ascending = False)
color = list_color(5)

plot = tmp.plot.pie(y='jumlah', title="persebaran produk berdasarkan gender", legend=False, \
                   autopct='%1.1f%%',  colors = color, \
                   shadow=True, startangle=0, figsize=(7, 7))

### Melihat persebaran produk berdasarkan kategori utama
tmp = pd.crosstab(index = product_df['masterCategory'],
            columns = 'jumlah produk').sort_values(by='jumlah produk', ascending = False)

plot = tmp.plot.bar(y='jumlah produk', figsize=(5, 5))

### Melihat persebaran produk berdasarkan kategori sekunder
for i in product_df.masterCategory.unique():
  tmp = product_df[product_df['masterCategory'] == i]
  tmp = pd.crosstab(index = tmp['subCategory'],
            columns = 'jumlah produk').sort_values(by='jumlah produk', ascending = True)
  plot = tmp.plot.barh(y='jumlah produk', \
                       title="persebaran produk berdasarkan kategori sekunder pada kategori utama " + i, \
                       legend=False, figsize=(7, 7))

product_df.articleType.nunique()

### top 10 produk yang tersedia, berdasarkan articleType
tmp = pd.crosstab(index = product_df['articleType'],
            columns = 'jumlah').sort_values(by='jumlah', ascending = False)
tmp = tmp.iloc[:10,:]
plot = tmp.plot.bar(y='jumlah', \
                    title="top 10 produk yang tersedia, berdasarkan articleType ", \
                    legend=False, figsize=(7, 11))

### persebaran produk berdasarkan articleType pada kategori utama
for i in product_df.masterCategory.unique():
  tmp = product_df[product_df['masterCategory'] == i]
  tmp = pd.crosstab(index = tmp['articleType'],
            columns = 'jumlah produk').sort_values(by='jumlah produk', ascending = True)
  plot = tmp.plot.barh(y='jumlah produk', \
                       title="persebaran produk berdasarkan articleType pada kategori utama " + i, \
                       legend=False, figsize=(7, 11))

product_df.baseColour.nunique()

### Melihat persebaran produk berdasarkan warna dasar produk
tmp = pd.crosstab(index = product_df['baseColour'],
            columns = 'jumlah').sort_values(by='jumlah', ascending = False)

plot = tmp.plot.bar(y='jumlah', \
                       title="persebaran produk berdasarkan baseColour", \
                       legend=False, figsize=(20, 7))

### Melihat season produk
tmp = pd.crosstab(index = product_df['season'],
            columns = 'jumlah').sort_values(by='jumlah', ascending = False)
color = list_color(5)

plot = tmp.plot.pie(y='jumlah', title="persebaran produk berdasarkan season produk", legend=False, \
                   autopct='%1.1f%%',  \
                   shadow=True, startangle=0,
                   figsize=(7, 7), colors = color)

### Melihat persebaran produk berdasarkan tahun produksi
tmp = pd.crosstab(index = product_df['year'],
            columns = 'jumlah produk')
plot = tmp.plot.bar(y='jumlah produk', figsize=(5, 5))

product_df.usage.unique()

label = ['Casual', 'Smart Casual', 'Ethnic', 'Travel', 'Formal', 'Party', 'Other', 'Sports', 'Home']
color = list_color(9)

fig, ax = plt.subplots(figsize=(10, 10))
ax.set_title('persebaran produk berdasarkan usage', fontsize=25, fontname="Helvetica")
#reindex(labels) sorts the index of the value counts according to the list labels
ax = product_df['usage'].value_counts(normalize=True).reindex(label).plot.pie(autopct='%1.0f%%',
                                                                              shadow=True,
                                                                              legend=True,
                                                                              colors = color)

ax.set_ylabel("")
plt.show()

### simpan
product_df.to_csv(raw_path + 'product_df1.csv', index= False)

"""## Customer"""

customer_df = pd.read_csv(raw_path + 'customer.csv')

customer_df.head()

customer_df.info()

"""tidak ada baris yang memuat nilai null.

### Deskripsi Kolom:
- customer_id : id unik customer <br>
- first_name : nama depan customer <br>
- last_name : nama belakang customer <br>
- username : username customer <br>
- email : surel customer <br>
- gender : jenis kelamin customer <br>
- birthdate : tanggal lahir customer <br>
- device_type : tipe device yang digunakan customer <br>
- device_id : id device customer <br>
- device_version : versi device yang digunakan customer <br>
- home_location_lat : latitude alamat customer <br>
- home_location_long : longitude alamat customer <br>
- home_location : provinsi domisili customer <br>
- home_country : negara domisili customer <br>
- first_join_date : tanggal bergabungnya customer <br>
"""

customer_df[["birthdate", "first_join_date"]] = customer_df[
    ["birthdate", "first_join_date"]
].astype("datetime64")

customer_df[["gender", "device_type", "home_location", "home_country"]] = customer_df[
    ["gender", "device_type", "home_location", "home_country"]
].astype("category")

"""### Visualisasi"""

### Melihat peningkatan jumlah customer berdasarkan tahun
customer_df['customer_id'].groupby(customer_df["first_join_date"].dt.year).count().plot(kind="bar", figsize=(6,6))

customer_df.head()

### Melihat persebaran customer berdasarka tipe device yang digunakannya
tmp = pd.crosstab(index=customer_df['device_type'],
            columns = 'jumlah').sort_values(by='jumlah', ascending = False)
plot = tmp.plot.pie(y='jumlah', figsize=(5, 5), colors = list_color(2),
                    autopct='%1.1f%%', shadow=True, legend=False,
                    title = 'persebaran customer berdasarka tipe device yang digunakannya')

tmp = pd.crosstab(index=customer_df['device_type'],
           columns=customer_df['gender'])
tmp.plot(kind='bar', figsize=(6,6))

### Melihat distribusi gender customer
tmp = pd.crosstab(index=customer_df['gender'],
            columns = 'jumlah').sort_values(by='jumlah', ascending = False)
plot = tmp.plot.pie(y='jumlah', figsize=(5, 5), colors = list_color(2),
                    autopct='%1.1f%%', shadow=True, legend=False,
                    title = 'persebaran customer berdasarka gendernya')

### melihat distribusi lokasi customer
tmp = customer_df['home_location'].value_counts()
tmp = tmp.sort_values(ascending=True)
tmp.plot(kind='barh', figsize=(10,10))

### Mengecek apakah ada customer dari luar negeri
tmp = pd.crosstab(index=customer_df['home_country'],
            columns = 'jumlah').sort_values(by='jumlah', ascending = False)
plot = tmp.plot.pie(y='jumlah', figsize=(5, 9))

### merge dan save
customer_df.nunique()

"""## Transactions"""

transactions_df = pd.read_csv(raw_path + 'transactions.csv')

transactions_df.head()

transactions_df.info()

"""### Deskripsi kolom:
- created_at : tanggal dilakukan pencatatan <br>
- customer_id : id unik customer <br>
- booking_id : id unik pembayaran yang telah dilakukan <br>
- session_id : id session <br>
- product_metadata : <br>
- payment_method : cara pembayaran <br>
- payment_status : status pembayaran (sukses / gagal) <br>
- promo_amount : banyaknya promo yang digunakan <br>
- promo_code : kode promo yang digunakan <br>
- shipment_fee : ongkos kirim produk <br>
- shipment_date_limit : tanggal deadline pengiriman produk <br>
- shipment_location_lat : latitude tujuan pengiriman produk <br>
- shipment_location_long : longitude tujuan pengiriman produk <br>
- total_amount : total biaya yang dikeluarkan customer dalam sebuah transaksi

### explode untuk menguraikan kolom product_metadata
"""

### cek dimensi transactions_df sebelum dilakukan explode
transactions_df.shape

### Mengubah tipe data kolom 'product_metadata' menjadi literal agar bisa
### dilakukan explode
tmp = transactions_df.iloc[:,4].astype('str')
tmp = tmp.map(lambda x: ast.literal_eval(x))
tmp = tmp.to_frame()
transactions_df['product_metadata'] = tmp['product_metadata']
transactions_df.shape

### explode
transactions_df = transactions_df.explode('product_metadata')
transactions_df.shape

transactions_df.info()

transactions_df.reset_index()

transactions_df.info()

tmp = pd.json_normalize(transactions_df.product_metadata)
tmp = tmp.reset_index()

transactions_df = pd. merge(transactions_df, tmp, left_index=True, right_index=True)

transactions_df.info()

transactions_df.drop(transactions_df.columns[4], axis=1, inplace=True)
transactions_df.drop(transactions_df.columns[13], axis=1, inplace=True)
transactions_df.head(2)

transactions_df[
    [
        "customer_id",
        "booking_id",
        "session_id",
        "payment_status",
        "promo_code",
        "product_id",
    ]
] = transactions_df[
    [
        "customer_id",
        "booking_id",
        "session_id",
        "payment_status",
        "promo_code",
        "product_id",
    ]
].astype(
    "category"
)

transactions_df[["created_at", "shipment_date_limit"]] = transactions_df[
    ["created_at", "shipment_date_limit"]
].astype("datetime64")

transactions_df.info()

"""### Handling missing values"""

transactions_df.isna().sum()

transactions_df.promo_code.unique()

"""### Visualisasi"""

### Melihat jumlah transaksi yang dibuat dari tahun ke tahun
transactions_df['created_at'].groupby(transactions_df["created_at"].dt.year).count().plot(kind="bar", figsize=(6,6))

### Melihat transaksi paling banyak dibuat pada bulan apa
transactions_df['created_at'].groupby(transactions_df["created_at"].dt.month).count().plot(kind="bar", figsize=(6,6))

### Produk mana yang paling laris terjual
pd.crosstab(index = transactions_df['product_id'],
            columns = 'total_penjualan',
            values =   transactions_df['quantity'],
            aggfunc = sum).sort_values(by='total_penjualan', ascending=False)

### Melihat top 20 produk terlaris
tmp = pd.crosstab(index = transactions_df['product_id'],
            columns = 'total_penjualan',
            values =   transactions_df['quantity'],
            aggfunc = sum).sort_values(by='total_penjualan', ascending=False)

tmp[:20].plot(kind='bar', figsize=(10,10), y='total_penjualan')

### Melihat perbandingan pembayaran berhasil dan gagal
tmp = pd.crosstab(index = transactions_df['payment_status'],
            columns = 'jumlah').sort_values(by='jumlah', ascending = False)

plot = tmp.plot.pie(y='jumlah', title="Status pembayaran produk", legend=False, \
                   autopct='%1.1f%%', colors = list_color(2), \
                   shadow=True, startangle=0, figsize=(7, 7))

### Customer paling banyak melakukan pembayaran melalui apa
tmp = pd.crosstab(index = transactions_df['payment_method'],
            columns = 'jumlah').sort_values(by='jumlah', ascending = False)
color = list_color(5)

plot = tmp.plot.pie(y='jumlah', title="metode pembayaran transaksi", legend=False, \
                   autopct='%1.1f%%', colors = color,  \
                   shadow=True, startangle=0, figsize=(7, 7))

### Customer paling banyak menggunakan kode promo apa?
tmp = pd.crosstab(index = transactions_df['promo_code'],
            columns = 'jumlah').sort_values(by='jumlah', ascending = False)
color = list_color(8)

plot = tmp.plot.pie(y='jumlah', title="kode promo yang digunakan dalam pembayaran transaksi", legend=False, \
                   autopct='%1.1f%%', colors = color, \
                   shadow=True, startangle=0, figsize=(7, 7))

"""### Aggregat data dan feature engineering"""

df = transactions_df.copy()

df.head()

df.info()

df.nunique()

## mulai tulis kode untuk feature engineering dari sini yaa

df['created_at'] = df['created_at'].astype('datetime64')
df['date'] = df['created_at'].dt.to_period('D')

df_agg = (
    df.groupby(["customer_id"])
    .agg(
        total_shipment=pd.NamedAgg(column="shipment_fee", aggfunc=sum),
        min_shipment=pd.NamedAgg(column="shipment_fee", aggfunc=min),
        max_shipment=pd.NamedAgg(column="shipment_fee", aggfunc=max),
        avg_shipment=pd.NamedAgg(column="shipment_fee", aggfunc="mean"),
        total_amount=pd.NamedAgg(column="total_amount", aggfunc=sum),
        min_amount=pd.NamedAgg(column="total_amount", aggfunc=min),
        max_amount=pd.NamedAgg(column="total_amount", aggfunc=max),
        avg_amount=pd.NamedAgg(column="total_amount", aggfunc="mean"),
        total_quantity=pd.NamedAgg(column="quantity", aggfunc=sum),
        min_quantity=pd.NamedAgg(column="quantity", aggfunc=min),
        max_quantity=pd.NamedAgg(column="quantity", aggfunc=max),
        avg_quantity=pd.NamedAgg(column="quantity", aggfunc="mean"),
        min_item_price=pd.NamedAgg(column="item_price", aggfunc=min),
        max_item_price=pd.NamedAgg(column="item_price", aggfunc=max),
        avg_item_price=pd.NamedAgg(column="item_price", aggfunc="mean"),
        frequency_trx=pd.NamedAgg(column="booking_id", aggfunc="nunique"),
        total_unique_product=pd.NamedAgg(column="product_id", aggfunc="nunique"),
        first_transaction=pd.NamedAgg(column="date", aggfunc=min),
        last_transaction=pd.NamedAgg(column="date", aggfunc=max),
    )
    .reset_index()
)

### menambahkan kolom basket size
df_agg['basket_size'] = round(df_agg['total_unique_product'] / df_agg['frequency_trx'])

df_agg.head(3)

df_agg.head()

"""### simpan

pada df_agg, telah dilakukan labeling
"""

## simpan
transactions_df.to_csv(raw_path + 'transactions_df.csv', index= False)
df_agg.to_csv(raw_path + 'df_agg1.csv', index= False)

"""## click_stream

* reset runtime, run cell pada block import, kemudian run cell pada block click_stream
"""

click_stream_df = pd.read_csv(raw_path + 'click_stream.csv')

click_stream_df.head()

click_stream_df.info()

"""### Deskripsi kolom:
- session_id : id session <br>
- event_name : event yang dilakukan <br>
- event_time : waktu event dilakukan <br>
- event_id : id event<br>
- traffic_source : sumber perangkat<br>
- event_metadata : data-data yang berhubungan dengan event<br>
"""

### membuang kolom yang tidak diperlukan
click_stream_df = click_stream_df.drop('event_id', axis=1)

click_stream_df['event_time'] = click_stream_df['event_time'].astype('datetime64')
click_stream_df['event_time'] = click_stream_df['event_time'].dt.to_period('D')

click_stream_df[["session_id", "event_name", "traffic_source"]] = click_stream_df[
    ["session_id", "event_name", "traffic_source"]
].astype("category")

click_stream_df.nunique()

click_stream_df.info()

type(click_stream_df.iloc[0,-1])

click_stream_df.isna().sum()

### Karena ada null value pada kolom event_metadata,
### maka explode dilakukan terpisah
click_stream_df1 = click_stream_df[click_stream_df.event_metadata.notna()]

#click_stream_df2 = click_stream_df[]

### ubah string menjadi literal
tmp = click_stream_df1.iloc[:,-1].astype('str')
tmp = tmp.map(lambda x: ast.literal_eval(x))
tmp = tmp.to_frame()
click_stream_df1['event_metadata'] = tmp['event_metadata']
click_stream_df1 = click_stream_df1.reset_index()

### uraikan kolom event_metadata
tmp = pd.json_normalize(click_stream_df1.event_metadata)
tmp = tmp.reset_index()
click_stream_df1 = pd. merge(click_stream_df1, tmp, left_index=True, right_index=True)

click_stream_df1.head(3)

### Gabungkan data
click_stream_df = click_stream_df1.append(click_stream_df[click_stream_df.event_metadata.isna()], ignore_index=True)
click_stream_df.drop(click_stream_df.columns[[0, 6]], axis=1, inplace=True)
click_stream_df.head()

### membuang kolom event_metadata
click_stream_df = click_stream_df.drop('event_metadata', axis=1)

click_stream_df.info()

click_stream_df.head()

click_stream_df.search_keywords.unique()

"""### Visualisasi"""

### Melihat persebaran event berdasarkan sumber device
tmp = pd.crosstab(index=click_stream_df['traffic_source'],
            columns = 'jumlah').sort_values(by='jumlah', ascending = False)
plot = tmp.plot.pie(y='jumlah', figsize=(5, 5), colors = list_color(2),
                    autopct='%1.1f%%', shadow=True, legend=False,
                    title = 'persebaran event berdasarkan sumber device')

click_stream_df.search_keywords.unique()

### Mencari tahu keyword mana yang paling banyak dicari
tmp = click_stream_df['search_keywords'].value_counts()
tmp = tmp.sort_values(ascending=True)
tmp.plot(kind='barh', figsize=(10,10), title='Keyword')

### Melihat persebaran click_stream berdasarkan event_name
tmp = pd.crosstab(index=click_stream_df['event_name'],
            columns = 'jumlah').sort_values(by='jumlah', ascending = False)
plot = tmp.plot.pie(y='jumlah', figsize=(5, 5), colors = list_color(9),
                    autopct='%1.1f%%', shadow=True, legend=False,
                    title = 'persebaran data berdasarkan event_name')

tmp = pd.crosstab(index=click_stream_df['event_name'],
           columns=click_stream_df['traffic_source'])
tmp.plot(kind='bar', figsize=(20,6), stacked=True,
         title = 'persebaran data berdasarkan event_name dan traffic_source')

"""### Aggregat data dan feature engineering"""

click_stream_df.info()

"""one-hot encoding :"""

for i in ['event_name',
          'traffic_source']:
          ### onehot click_stream_df
          one_hot = pd.get_dummies(click_stream_df[i])
          # Drop column i as it is now encoded
          click_stream_df = click_stream_df.drop(i,axis = 1)
          # Join the encoded click_stream_df
          click_stream_df = click_stream_df.join(one_hot)

click_stream_df.info()

click_stream_df.head(2)

click_stream_df.columns

def unique_non_null(s):
    return s.dropna().unique()

### Agg
df_agg2 = (
    click_stream_df.groupby(["session_id"])
    .agg(
        n_ADD_PROMO = pd.NamedAgg(column="ADD_PROMO", aggfunc=sum),
        n_ADD_TO_CART = pd.NamedAgg(column="ADD_TO_CART", aggfunc=sum),
        n_BOOKING = pd.NamedAgg(column="BOOKING", aggfunc=sum),
        n_CLICK = pd.NamedAgg(column="CLICK", aggfunc=sum),
        n_HOMEPAGE = pd.NamedAgg(column="HOMEPAGE", aggfunc=sum),
        n_ITEM_DETAIL = pd.NamedAgg(column="ITEM_DETAIL", aggfunc=sum),
        n_PROMO_PAGE = pd.NamedAgg(column="PROMO_PAGE", aggfunc=sum),
        n_SCROLL = pd.NamedAgg(column="SCROLL", aggfunc=sum),
        n_SEARCH = pd.NamedAgg(column="SEARCH", aggfunc=sum),
        MOBILE = pd.NamedAgg(column='MOBILE', aggfunc='mean'),
        WEB = pd.NamedAgg(column='WEB', aggfunc='mean')

    )
    .reset_index()
)

df_agg2.head()



"""### simpan"""

## simpan
click_stream_df.to_csv(raw_path + 'click_stream_df.csv', index= False)
df_agg2.to_csv(raw_path + 'df_agg2.csv', index= False)

"""# 2. Churn

## Merge

Akan dilakukan penggabungan tabel-tabel untuk mendapatkan tabel yang akan digunakan dalam machine learning
"""

transactions_df = pd.read_csv(raw_path + 'transactions_df.csv')
customer_df = pd.read_csv(raw_path + 'customer.csv')

df_trans = transactions_df[transactions_df['payment_status'] == 'Success']

df_trans.customer_id.nunique()

customer_df.customer_id.nunique()

presentase = 100 * df_trans.customer_id.nunique() / customer_df.customer_id.nunique()
print("Sebesar "+str(presentase)+" % customer terdaftar telah melakukan transaksi dari total "\
      +str(customer_df.customer_id.nunique())+" customer")

"""Ada 50242 unique customer yang tercatat melakukan transaksi, sedangkan pada dataset, tercatat 100000 unique customer. Ini menunjukkan bahwa terdapat 49758 customer yang hanya mendaftarkan akun saja dan belum bertransaksi atau status pembayaran dalam transaksinya masih belum "Success"."""

click_stream_df = pd.read_csv(raw_path + 'click_stream_df.csv')

click_stream_df = click_stream_df[click_stream_df['payment_status'] == 'Success']
#click_stream_df.head(3)

click_stream_df = click_stream_df[['session_id',
                                   'product_id',
                                   'search_keywords',
                                	 'MOBILE',
                                	 'WEB',
                                ]]
click_stream_df.head(3)

click_stream_df.shape, click_stream_df.session_id.nunique()

df_trans.shape, df_trans.session_id.nunique()

df_trans = df_trans.merge(click_stream_df, on='session_id', how='inner')
df_trans.shape, df_trans.session_id.nunique()

customer_df.customer_id = customer_df.customer_id.astype('category')
df_trans.customer_id = df_trans.customer_id.astype('category')

customer_df = customer_df[['customer_id', 'gender', 'birthdate', 'device_type', 'first_join_date']]
customer_df.head()

df_trans = df_trans.merge(customer_df, on='customer_id', how='left')
df_trans.shape

df_trans.head(3)

df_trans[['created_at', 'birthdate', 'first_join_date']] = \
  df_trans[['created_at', 'birthdate', 'first_join_date']].astype('datetime64')

#get yearmonth
df_trans['yearmonth'] = df_trans['created_at'].dt.strftime('%Y-%m')

### to period
for i in ['created_at', 'birthdate', 'first_join_date']:
  df_trans[i] = df_trans[i].dt.to_period('D')

### mendefinisikan max_date := tanggal terakhir aktivitas transaksi pada dataset
max_date = max(df_trans['created_at']+ 1)
max_date

### menambahkan kolom customer_age := usia customer (dalam hari)
df_trans['customer_age'] = max_date - df_trans['birthdate']

### menambahkan kolom account_age := usia akun customer (dalam hari)
df_trans['account_age'] = max_date - df_trans['first_join_date']

### regex untuk merapikan kolom-kolom yang baru saja dibuat
import re
tmp = ['customer_age', 'account_age']

for i in tmp:
  df_trans[i] =   df_trans[i].astype('string')
  df_trans[i] =   df_trans[i].apply(lambda x : re.sub("Days", " ", (x)))
  df_trans[i] =   df_trans[i].apply(lambda x : re.sub("<", " ", (x)))
  df_trans[i] =   df_trans[i].apply(lambda x : re.sub(">", " ", (x)))

  df_trans[i] =   df_trans[i].apply(lambda x : re.sub("\s", "", (x)))
  df_trans[i] =   df_trans[i].apply(lambda x : x.replace("*", " "))

  df_trans[i] =   df_trans[i].astype('string')
  df_trans[i] =   df_trans[i].apply(lambda x : re.sub("Days", " ", (x)))
  df_trans[i] =   df_trans[i].apply(lambda x : re.sub("<", " ", (x)))
  df_trans[i] =   df_trans[i].apply(lambda x : re.sub(">", " ", (x)))

  df_trans[i] =   df_trans[i].apply(lambda x : re.sub("\s", "", (x)))
  df_trans[i] =   df_trans[i].apply(lambda x : x.replace("*", " "))

df_trans.head()

#select feature
df_feature = df_trans[['created_at',
           'customer_id',
           'yearmonth',
           'promo_amount',
           'promo_code',
           'shipment_fee',
           'total_amount',
           'quantity',
           'item_price',
           'gender',
           'device_type',
           'customer_age',
           'account_age',
           'search_keywords',
           ]]

for i in ['gender', 'device_type', 'promo_code']:
          ### onehot
          one_hot = pd.get_dummies(df_feature[i])
          # Drop column i as it is now encoded
          df_feature = df_feature.drop(i,axis = 1)
          # Join the encoded
          df_feature = df_feature.join(one_hot)

df_feature.head()

#check min max of month
print(min(df_feature['yearmonth']))
print(max(df_feature['yearmonth']))

df_feature

promo = ['AZ2022', 'BUYMORE', 'LIBURDONG', 'SC2022',
         'STARTUP', 'WEEKENDSERU', 'XX2022', 'WEEKENDMANTAP']
df_feature['promo_used'] = df_feature['AZ2022'] + df_feature['BUYMORE'] + df_feature['LIBURDONG']\
                     + df_feature['STARTUP'] + df_feature['WEEKENDSERU'] + df_feature['XX2022']\
                     + df_feature['WEEKENDMANTAP'] + df_feature['SC2022']
df_feature = df_feature.drop(columns = promo)
df_feature

# agg level for feature
df_agg = df_feature.groupby(["yearmonth", "customer_id"], as_index=False)[
    "promo_amount",
    "shipment_fee",
    "total_amount",
    "quantity",
    "item_price",
    "customer_age",
    "account_age",
    "F",
    "M",
    "Android",
    "iOS",
].mean()
df_agg2 = df_feature.groupby(["yearmonth", "customer_id"], as_index=False)[
    "promo_used"
].sum()
df_agg = df_agg.merge(df_agg2, on=['yearmonth', 'customer_id'], how='inner')

#create freq of trx and merge
df_agg2 = df_feature.groupby(['yearmonth','customer_id'], as_index=False)['created_at'].nunique()
df_agg2 = df_agg2.rename(columns = {'created_at':'freq_trx'})

df_feature = df_agg2.merge(df_agg, how='left', on=['yearmonth','customer_id'])

#data actual
data_min = min(df_trans['created_at'])
data_max = max(df_trans['created_at'])

print("Seluruh transaksi yang tercatat berada dalam rentang waktu {min} hingga {max}".format(min = data_min, max = data_max))

"""Akan diberi label churn untuk dataset yang berada dalam rentang waktu juni 2016 hingga juni 2022. Data pada bulan juli 2022 akan dipakai untuk memprediksi status churn customer pada bulan agustus 2022."""

label_max = str(data_max)[:-3]
print(label_max)

df_feature_model = df_feature[df_feature.yearmonth != label_max]
df_feature_prediksi = df_feature[df_feature.yearmonth == label_max]

"""## Label churn dengan time window 1 bulan"""

#create func for label churn
def label(df):
    if (df['yearmonth_x'] == 0) | (df['yearmonth_y'] == 0):
        return 1 #1 adalah customer label churn
    elif (df['yearmonth_x'] != 0) & (df['yearmonth_y'] != 0):
        return 0 #0 adalah customer level tidak churn
    else:
        return " "

df_feature_model.head()

df_feature_model['year'] = df_feature_model['yearmonth'].astype('datetime64').dt.strftime('%Y')

### membuat list yang memuat df_feature_model berdasarkan tahunnya
df_feat = []
TAHUN = [2016, 2017, 2018, 2019, 2020, 2021, 2022]
for tahun in TAHUN:
  df_feat.append(df_feature_model[df_feature_model.year == str(tahun)])

### quick check
### df_feat[0] == df_feature yang tahunnya = 2016, dst..
for i in range(len(df_feat)):
  print(df_feat[i].shape, df_feature_model[df_feature_model.year == str(TAHUN[i])].shape)

### final_dataset akan berisi label churn untuk setiap periode satu bulan
final_dataset = []

# lakukan looping hingga tahun ke n-1
# (jika tahun terakhir dataset adalah n=2022, looping berikut hanya bekerja untuk
#  data hingga tahun n-1 = 2021, data tahun 2022 dilabeli terpisah)
for tahun in range(len(df_feat) - 1):
    # df_feat merupakan df_feature pada suatu tahun iterasi,
    # df_next untuk tahun iterasi berikutnya
    df = df_feat[tahun]
    if tahun+1 <len(df_feat):
      df_next = df_feat[tahun+1]
    # month merupakan list yg berisi df berdasarkan bulannya
    month = []
    for bulan in range(12):
      month.append(df[df.yearmonth == str(TAHUN[tahun]) + '-'+ str(bulan + 1).zfill(2)])

    # flag merupakan list yg akan diisi dengan df yang digabung bulan berikutnya
    flag = []
    # flag untuk bulan 1-11
    for bulan in range(11):
      flag.append(pd.merge(month[bulan], month[bulan + 1][['customer_id', 'yearmonth']], on='customer_id', how='outer'))
      flag[bulan].fillna(0, inplace = True)
    # flag untuk bulan 12 (merge bulan desember dengan januari)
    month.append(df_next[df_next.yearmonth == str(TAHUN[tahun]) + '-01'])
    flag.append(pd.merge(month[11], month[12][['customer_id', 'yearmonth']], on='customer_id', how='outer'))
    flag[11].fillna(0, inplace = True)

    dataset = pd.concat(flag)
    dataset['churn'] = dataset.apply(label, axis = 1)
    final_dataset.append(dataset)

# melabeli data tahun terakhir (=2022)
df = df_feat[-1]
df['month'] = df['yearmonth'].apply(lambda x: str(x)[5:])
max_month = max(df.month)
month = []
for bulan in df.month.unique():
  month.append(df[df.month == bulan])
flag = []
# flag hingga bulan max_month-1
for bulan in range(len(month)-1):
  flag.append(pd.merge(month[bulan], month[bulan + 1][['customer_id', 'yearmonth']], on='customer_id', how='outer'))
  flag[bulan].fillna(0, inplace = True)
# flag untuk bulan max_month
flag.append(pd.merge(month[-1], df_feature_prediksi[['customer_id', 'yearmonth']], on='customer_id', how='outer'))
flag[-1].fillna(0, inplace = True)
dataset = pd.concat(flag)
dataset = dataset.drop(columns = 'month')
dataset['churn'] = dataset.apply(label, axis = 1)
final_dataset.append(dataset)

final_dataset = pd.concat(final_dataset)

final_dataset

"""## EDA churn"""

customer_df = pd.read_csv(raw_path + 'customer.csv')

final_dataset_join_customer = pd.merge(final_dataset, customer_df, on='customer_id', how='left')

final_dataset_join_customer.info()

### Melihat persentase status churn
tmp = pd.crosstab(index=final_dataset_join_customer['churn'],
            columns = 'jumlah').sort_values(by='jumlah', ascending = False)
plot = tmp.plot.pie(y='jumlah', figsize=(5, 5), colors = list_color(2),
                    autopct='%1.1f%%', shadow=True, legend=False,
                    title = 'persebaran churn')

tmp = pd.crosstab(index=final_dataset_join_customer['churn'],
           columns=final_dataset_join_customer['gender'])
a = tmp.plot(kind='pie', figsize=(10,10), subplots=True,
             autopct='%1.1f%%', colors = list_color(2))
tmp = pd.crosstab(index=final_dataset_join_customer['gender'],
           columns=final_dataset_join_customer['churn'])
b = tmp.plot(kind='pie', figsize=(10,10), subplots=True,
             autopct='%1.1f%%', colors = list_color(2))
print(a,b)

final_dataset_join_customer.home_location = final_dataset_join_customer.home_location.astype('category')

tmp = pd.crosstab(index=final_dataset_join_customer['home_location'],
           columns=final_dataset_join_customer['churn'])

tmp = tmp.sort_values(by=1,ascending=False)
tmp.plot(kind='bar', figsize=(15, 10))

final_dataset.to_csv(raw_path + 'df_model_dengan_yearmonth.csv')

df_model = final_dataset.drop(['customer_id','yearmonth_x','yearmonth_y', 'year'], axis=1)

### Save model dataset (untuk machine learning dan visualisasi)
final_dataset_join_customer.to_csv(raw_path + 'final_dataset_join_customer.csv')
final_dataset.to_csv(raw_path + 'df_final.csv', index= False) #dataset yang telah dilabeli churn
df_model.to_csv(raw_path + 'df_model.csv', index= False) #final dataset tanpa customer_id
df_feature_prediksi.to_csv(raw_path + 'df_prediksi.csv', index= False) #berisi fitur2 bulan juli 2022, akan diprediksi status churn untuk agustus 2022

"""# 3. Machine Learning

## cek data
"""

# load
df_model = pd.read_csv(raw_path + 'df_model.csv')
df_prediksi = pd.read_csv(raw_path + 'df_prediksi.csv')

#check data
def missing_check(df):
    missing=df.isnull().sum()
    per_mis = 100*(missing/len(df))
    data_type = df.dtypes
    number_unique = df.nunique()
    return pd.DataFrame({"Missing" : missing,
                         "Percent_Missing" : per_mis,
                         "Data_Types" : data_type,
                         "Number_Unique" : number_unique
                        })
missing_check(df_model)

df_model.head()

df_model.shape

df_model.info()

df_model.describe()

sns.heatmap(df_model.corr())

"""## Split data"""

#data validation into split train & test
y= df_model['churn'] #target variable
X= df_model.drop(['churn'],1)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=100)

def distplot(feature, frame, color='r'):
    plt.figure(figsize=(8,3))
    plt.title("Distribution for {}".format(feature))
    ax = sns.distplot(frame[feature], color= color)

df_model

#check distribution
num = list(df_model.select_dtypes(exclude=['object', 'category']).columns) #numerical type columns
for feat in num: distplot(feat, df_model)

num.remove('churn')

#check outliers df_model
df_model.plot(kind="box", subplots=True, layout=(10,4), figsize=(10,50))

X_train

"""## Baseline Model

### Naive Bayes
"""

#naive bayes model
model_nb = GaussianNB()
model_nb.fit(X_train, y_train)

# Make predictions
predictnb_y = model_nb.predict(X_test)
accuracy_nb = model_nb.score(X_test,y_test)
print("Naive Bayes accuracy is :",accuracy_nb)

print(classification_report(y_test, predictnb_y))

#auc score
auc = metrics.roc_auc_score(y_test,predictnb_y)
print(auc)

actual = y_test
predicted = predictnb_y

confusion_matrix = metrics.confusion_matrix(actual, predicted)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])

cm_display.plot()
plt.show()

#ROC
y_nbpred_prob = model_nb.predict_proba(X_test)[:,1]
fpr_rf, tpr_rf, thresholds = roc_curve(y_test, y_nbpred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr_rf, tpr_rf, label='Random Forest',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Naive Bayes ROC Curve',fontsize=16)
plt.show();

"""### Logistic Regression"""

#logistic regression model
model_lr = LogisticRegression()
model_lr.fit(X_train, y_train)

# Make predictions
predictlr_y = model_lr.predict(X_test)
accuracy_lr = model_lr.score(X_test,y_test)
print("Logistic Regression accuracy is :",accuracy_lr)

#auc score
auc = metrics.roc_auc_score(y_test,predictlr_y)
print(auc)

print(classification_report(y_test, predictlr_y))

#ROC
y_lrpred_prob = model_lr.predict_proba(X_test)[:,1]
fpr_rf, tpr_rf, thresholds = roc_curve(y_test, y_lrpred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr_rf, tpr_rf, label='Random Forest',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Logistic Regression ROC Curve',fontsize=16)
plt.show();

actual = y_test
predicted = predictlr_y

confusion_matrix = metrics.confusion_matrix(actual, predicted)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])

cm_display.plot()
plt.show()

"""### Decision Tree"""

#decision Tree model
model_dt = DecisionTreeClassifier()
model_dt.fit(X_train,y_train)
predictdt_y = model_dt.predict(X_test)
accuracy_dt_train = model_dt.score(X_train,y_train)
accuracy_dt_test = model_dt.score(X_test,y_test)
print("Decision Tree accuracy data train :",accuracy_dt_train)
print("Decision Tree accuracy data test :", accuracy_dt_test)

#auc score
auc = metrics.roc_auc_score(y_test,predictdt_y)
print(auc)

print(classification_report(y_test, predictdt_y))

actual = y_test
predicted = predictdt_y

confusion_matrix = metrics.confusion_matrix(actual, predicted)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])

cm_display.plot()
plt.show()

#ROC
y_dtpred_prob = model_dt.predict_proba(X_test)[:,1]
fpr_rf, tpr_rf, thresholds = roc_curve(y_test, y_dtpred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr_rf, tpr_rf, label='Random Forest',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Decision Tree ROC Curve',fontsize=16)
plt.show();

"""## Using others model"""

#function for metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import roc_curve, auc

def get_metrics(y_pred, y_act, model_name):
    recall =  recall_score(y_act, y_pred)
    precision = precision_score(y_act, y_pred)
    roc_auc = roc_auc_score(y_act, y_pred)
    f1 = f1_score(y_act, y_pred)
    accuracy = balanced_accuracy_score(y_act, y_pred)

    metrics_result = dict(model_name=model_name,
                        recall=recall,
                        precision=precision,
                        roc_auc=roc_auc,
                        f1=f1,
                        accuracy=accuracy)

    return metrics_result

#funciton for prediction
def prediction(x_train, y_train, x_test, y_test, clf, model_name):

    clf = clf
    clf.fit(x_train, y_train)
    y_test_pred = clf.predict(x_test)
    y_train_pred = clf.predict(x_train)


    result = dict()
    test_result = get_metrics(y_pred=y_test_pred, y_act=y_test, model_name=model_name)
    train_result = get_metrics(y_pred=y_train_pred, y_act=y_train, model_name=model_name)

    result['test'] = test_result
    result['train'] = train_result
    result['y_test'] = y_test
    result['y_test_pred'] = y_test_pred
    result['y_train'] = y_train
    result['y_train_pred'] = y_train_pred
    result['clf'] = clf
    result['x_train'] = x_train
    result['x_test'] = x_test

    del clf

    return result

#function for hyperparameter
def show_best_hyperparameter(model, hyperparameters):
    for key, value in hyperparameters.items() :
        print('Best '+key+':', model.get_params()[key])

#function for feature importance
def show_feature_importance(model):
    feat_importances = pd.Series(model.feature_importances_, index=x.columns)
    ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
    ax.invert_yaxis()

    plt.xlabel('score')
    plt.ylabel('feature')
    plt.title('feature importance score')

#@title install
 !pip install shap --quiet
 !pip install feature_engine --quiet
 print('Ok!')

# Commented out IPython magic to ensure Python compatibility.
#Import ALL Package
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from matplotlib import rcParams

#Pre-Processing
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn import tree
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, balanced_accuracy_score
from sklearn.metrics import classification_report
from feature_engine.encoding import OneHotEncoder
from feature_engine.encoding import OrdinalEncoder

#Modeling
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import AdaBoostClassifier
import xgboost as xgb

#feature importance
import shap

"""## Model Prediction"""

model_1_result = prediction(X_train, y_train, X_test, y_test, xgb.XGBClassifier(random_state=42), model_name='xgboost')
model_2_result = prediction(X_train, y_train, X_test, y_test, RandomForestClassifier(random_state=42), model_name='random forest')
model_3_result = prediction(X_train, y_train, X_test, y_test, LogisticRegression(random_state=42), model_name='logistic regression')
model_4_result = prediction(X_train, y_train, X_test, y_test, DecisionTreeClassifier(random_state=42), model_name='decision tree')

"""## Model in Train"""

pd.DataFrame([model_1_result['train'],
              model_2_result['train'],
              model_3_result['train'],
              model_4_result['train'],
              ])

"""## Model in Test"""

pd.DataFrame([model_1_result['test'],
              model_2_result['test'],
              model_3_result['test'],
              model_4_result['test'],
              ])

"""## Hyperparameter Tuning

Penyesuaian hyperparameter adalah bagian penting untuk mengendalikan perilaku model. Jika tidak menyetel/mentuning hyperparameter dengan benar, estimasi parameter model akan menghasilkan hasil yang kurang optimal, karena tidak meminimalkan loss function. Dengan itu model membuat lebih banyak error. Dalam praktiknya, key indicators seperti akurasi atau confusion matrix akan lebih buruk
https://www.anyscale.com/blog/what-is-hyperparameter-tuning

### Logistic Regression
"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

# List Hyperparameters yang akan diuji
penalty = ['l1', 'l2']
C = np.logspace(-4,4,20) # Inverse of regularization strength; smaller values specify stronger regularization.
hyperparameters = dict(penalty=penalty, C=C)

# Inisiasi model
logres = LogisticRegression(random_state=42) # Init Logres dengan Gridsearch, cross validation = 5
model_logres_ht = RandomizedSearchCV(logres, hyperparameters, cv=5, random_state=42, scoring='recall')

# Fitting Model & Evaluation
logres_hyper = prediction(X_train, y_train, X_test, y_test, model_logres_ht, model_name='logistic regression_hyperparam')

show_best_hyperparameter(model_logres_ht.best_estimator_, hyperparameters)

print('Train_score:' + str(model_logres_ht.score(X_train,y_train))) #recall score
print('Test_score:' + str(model_logres_ht.score(X_test,y_test))) #recall

"""### Decision Tree"""

from feature_engine.discretisation import decision_tree
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

# List Hyperparameters yang akan diuji
max_depth = [2,3,4,6,8,10,12] #Kedalaman tree
max_leaf_nodes = [10,20,25] #leaf nodes
criterion = ['gini', 'entropy']
hyperparameters = dict(max_depth=max_depth, max_leaf_nodes=max_leaf_nodes, criterion=criterion)

# Inisiasi model
decision_tree = DecisionTreeClassifier(random_state=42) # Init Logres dengan Gridsearch, cross validation = 5
model_dectree_ht = RandomizedSearchCV(decision_tree, hyperparameters, cv=5, random_state=42, scoring='recall')

# Fitting Model & Evaluation
dectree_model = prediction(X_train, y_train, X_test, y_test, model_dectree_ht, model_name='decision tree_hyperparam')

show_best_hyperparameter(model_dectree_ht.best_estimator_, hyperparameters)

print('Train_score:' + str(model_dectree_ht.score(X_train,y_train))) #recall score
print('Test_score:' + str(model_dectree_ht.score(X_test,y_test))) #recall

"""### Result of Hyperparamter Train"""

pd.DataFrame([logres_hyper['train'], dectree_model['train']])

"""### Result of Hyperparamter Test"""

pd.DataFrame([logres_hyper['test'], dectree_model['test']])

"""Tim kami memilih model logistic regression karena model ini memilik roc_auc dan akurasi yang paling tinggi.

## Feature Importance
"""

explainer = shap.Explainer(logres_hyper['clf'].predict, logres_hyper['x_test'])
shap_values_cat = explainer(logres_hyper['x_test'])

shap.plots.bar(shap_values_cat)

"""#### penjelasan
Top 5 fitur terpenting dalam model ini yaitu <br>
1. Harga barang
2. Pembayaran
3. Ongkos kirim
4. Potongan harga dari promo
5. Frekuensi transaksi per bulan
6. Jumlah promo yang digunakan
"""

### final dataset
df_model_dengan_yearmonth = pd.read_csv(raw_path +'df_model_dengan_yearmonth')
df_final_model = df_model_dengan_yearmonth.copy()
df_final_model = df_final_model.drop(columns = ['Unnamed: 0', 'yearmonth_x'])
df_final_model

aktual = df_final_model['churn']
yearmonth = df_final_model['yearmonth_y']
customerid = df_final_model['customer_id']
df_final_model = df_final_model.drop(columns = ['churn', 'customer_id', 'yearmonth_y', 'year'])
prediksi = model_logres_ht.predict(df_final_model)
df_final_model['yearmonth'] = pd.DataFrame(yearmonth)
df_final_model['customer_id'] = pd.DataFrame(customerid)
df_final_model['prediksi'] = pd.DataFrame(prediksi)
df_final_model['aktual'] = pd.DataFrame(aktual)
df_final_model

### prediksi status churn customer untuk bulan agustus 2022
df_final_prediksi = df_prediksi.copy()
yearmonth = df_final_prediksi['yearmonth']
customerid = df_final_prediksi['customer_id']
df_final_prediksi = df_final_prediksi.drop(columns = ['yearmonth', 'customer_id'])
prediksi = model_logres_ht.predict(df_final_prediksi)
df_final_prediksi['yearmonth'] = pd.DataFrame(yearmonth)
df_final_prediksi['customer_id'] = pd.DataFrame(customerid)
df_final_prediksi['prediksi'] = pd.DataFrame(prediksi)
df_final_prediksi

df_final_prediksi[df_final_prediksi.prediksi == 1]

df_final_prediksi[df_final_prediksi.prediksi == 1]

"""Dengan model ini, diprediksi sejumlah 101 customer dari 9974 customer yang bertransaksi di bulan juli 2022 akan berstatus churn pada bulan agustus 2022"""

### Dengan asumsi bahwa customer yang tidak churn akan kembali bertransaksi
### dengan total amount yang sama, maka perusahaan akan lost revenue untuk
### customer yang berstatus churn.
df_final_prediksi['lost_revenue'] = df_final_prediksi['prediksi'] * df_final_prediksi['total_amount']
df_final_prediksi

presentase = 100 * df_final_prediksi.lost_revenue.sum() / df_final_prediksi.total_amount.sum()
print("Lost revenue bulan agustus 2022 yaitu sebesar "+str(presentase)+" % dari dari total amount customer sebesar "\
      +str(df_final_prediksi.total_amount.sum())+" satuan")

### Save
df_final_model.to_csv(raw_path + 'df_final_model_churn.csv')
df_final_prediksi.to_csv(raw_path + 'df_final_prediksi_churn.csv')

"""## Save Fix Model"""

import pickle
#save model
filename = "fix_model.sav"
pickle.dump(logres_hyper, open(filename, 'wb'))